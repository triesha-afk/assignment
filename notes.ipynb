# Cell 1 - Markdown
"""
# üìä Python for Business & Machine Learning - Complete Guide

---

## Table of Contents
1. [Why Python for Business and ML](#section1)
2. [Data Visualization](#section2)
3. [Data Extraction, Loading, Wrangling & Visualization](#section3)

---
"""

# Cell 2 - Markdown
"""
<a id='section1'></a>
# 1. Why Python for Business and ML

## Overview
Python has become the **dominant language** for business analytics and machine learning due to its simplicity, versatility, and powerful ecosystem.

---

## üéØ Key Reasons to Use Python

### **Ease of Learning**
- <span style="background-color: #E6D5FF; padding: 2px 8px; border-radius: 3px;">**Simple syntax**</span> that reads like English
- Great for beginners and non-programmers
- Shorter development time compared to Java or C++

### **Rich Ecosystem**
Python offers specialized libraries for every business need:

| **Domain** | **Popular Libraries** |
|------------|----------------------|
| Data Analysis | Pandas, NumPy |
| Visualization | Matplotlib, Seaborn, Plotly |
| Machine Learning | Scikit-learn, TensorFlow, PyTorch |
| Web Scraping | BeautifulSoup, Scrapy |
| Automation | Selenium, PyAutoGUI |

### **Industry Adoption**
- <span style="background-color: #E6D5FF; padding: 2px 8px; border-radius: 3px;">**Google, Netflix, NASA**</span> use Python extensively
- #1 language for data science and ML roles
- Strong community support with 10M+ developers

---

## üíº Business Applications

### Financial Analysis
```python
# Simple ROI calculation
investment = 10000
returns = 12500
roi = ((returns - investment) / investment) * 100
print(f"ROI: {roi}%")
```

### Customer Segmentation
- Analyze customer behavior patterns
- Predict churn and lifetime value
- Personalize marketing campaigns

### Process Automation
- <span style="background-color: #E6D5FF; padding: 2px 8px; border-radius: 3px;">**Automate repetitive tasks**</span> (reports, emails, data entry)
- Save hundreds of hours annually
- Reduce human error

---

## ü§ñ Machine Learning with Python

### What is Machine Learning?
> Machine Learning is the science of getting computers to learn and act like humans, improving automatically through experience.

### Common ML Applications
1. **Predictive Analytics** - Sales forecasting, demand prediction
2. **Classification** - Email spam detection, customer categorization
3. **Recommendation Systems** - Product suggestions, content recommendations
4. **Natural Language Processing** - Sentiment analysis, chatbots

### Why Python for ML?
- <span style="background-color: #E6D5FF; padding: 2px 8px; border-radius: 3px;">**Scikit-learn**</span> provides ready-to-use ML algorithms
- Easy integration with data processing libraries
- Excellent visualization capabilities for model interpretation

---

## ‚úÖ Key Takeaways
- Python combines simplicity with powerful capabilities
- Extensive libraries reduce development time
- Industry standard for data science and ML
- Suitable for both technical and business users

---
"""

# Cell 3 - Markdown
"""
<a id='section2'></a>
# 2. Data Visualization

## Overview
Data visualization transforms complex datasets into <span style="background-color: #E6D5FF; padding: 2px 8px; border-radius: 3px;">**visual stories**</span> that drive business insights and decision-making.

---

## üìà Why Visualize Data?

### The Power of Visual Communication
- **Human brain processes images 60,000x faster** than text
- Patterns become immediately obvious
- Facilitates stakeholder communication
- Enables data-driven decision making

### From Numbers to Insights
```
Raw Data: [23, 45, 67, 34, 89, 12, 56, 78]
‚Üì
Visualization: Line chart showing upward trend
‚Üì
Insight: 40% growth in Q3 sales
```

---

## üé® Core Visualization Libraries

### **Matplotlib** - The Foundation
- <span style="background-color: #E6D5FF; padding: 2px 8px; border-radius: 3px;">**Most widely used**</span> plotting library
- Complete control over every element
- Great for publication-quality figures
```python
import matplotlib.pyplot as plt

# Basic line plot
sales = [120, 135, 158, 167, 190]
months = ['Jan', 'Feb', 'Mar', 'Apr', 'May']

plt.figure(figsize=(10, 6))
plt.plot(months, sales, marker='o', linewidth=2, color='purple')
plt.title('Monthly Sales Trend', fontsize=16, fontweight='bold')
plt.xlabel('Month')
plt.ylabel('Sales ($1000s)')
plt.grid(True, alpha=0.3)
plt.show()
```

### **Seaborn** - Statistical Visualization
- Built on top of Matplotlib
- <span style="background-color: #E6D5FF; padding: 2px 8px; border-radius: 3px;">**Beautiful default styles**</span>
- Specialized for statistical plots
```python
import seaborn as sns
import pandas as pd

# Create sample data
data = pd.DataFrame({
    'Category': ['A', 'B', 'C', 'D', 'E'],
    'Values': [23, 45, 56, 78, 34]
})

# Bar plot with Seaborn
plt.figure(figsize=(10, 6))
sns.barplot(x='Category', y='Values', data=data, palette='Purples')
plt.title('Category Performance', fontsize=16, fontweight='bold')
plt.show()
```

### **Plotly** - Interactive Visualizations
- Creates interactive, web-ready charts
- Hover tooltips, zoom, pan capabilities
- <span style="background-color: #E6D5FF; padding: 2px 8px; border-radius: 3px;">**Professional dashboards**</span>
```python
import plotly.express as px

# Interactive scatter plot
df = px.data.iris()
fig = px.scatter(df, x='sepal_width', y='sepal_length', 
                 color='species', size='petal_length',
                 title='Iris Dataset Analysis')
fig.show()
```

---

## üìä Common Chart Types & Use Cases

### 1. **Line Charts**
- **Use Case**: Trends over time
- **Example**: Stock prices, website traffic, sales trends
```python
plt.plot(dates, values)
```

### 2. **Bar Charts**
- **Use Case**: Comparing categories
- **Example**: Regional sales, product performance
```python
plt.bar(categories, values)
```

### 3. **Scatter Plots**
- **Use Case**: Relationships between variables
- **Example**: Height vs weight, price vs demand
- <span style="background-color: #E6D5FF; padding: 2px 8px; border-radius: 3px;">**Reveals correlations**</span>
```python
plt.scatter(x_values, y_values)
```

### 4. **Histograms**
- **Use Case**: Distribution of data
- **Example**: Age distribution, salary ranges
```python
plt.hist(data, bins=20)
```

### 5. **Heatmaps**
- **Use Case**: Correlation matrices, intensity maps
- **Example**: Feature correlations, geographic data
```python
sns.heatmap(correlation_matrix, annot=True)
```

### 6. **Box Plots**
- **Use Case**: Statistical distribution, outlier detection
- **Example**: Salary by department, test scores by class
```python
sns.boxplot(x='category', y='values', data=df)
```

---

## üéØ Visualization Best Practices

### Design Principles
1. **Choose the right chart** for your data type
2. **Keep it simple** - avoid chart junk
3. **Use color purposefully** - <span style="background-color: #E6D5FF; padding: 2px 8px; border-radius: 3px;">**highlight key insights**</span>
4. **Label clearly** - titles, axes, legends
5. **Consider your audience** - technical vs non-technical

### Common Mistakes to Avoid
‚ùå 3D charts that distort perception  
‚ùå Too many colors or patterns  
‚ùå Missing axis labels  
‚ùå Misleading scales  
‚ùå Cluttered visuals  

---

## üí° Advanced Visualization Techniques

### Subplots - Multiple Charts
```python
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

axes[0, 0].plot(x1, y1)
axes[0, 0].set_title('Chart 1')

axes[0, 1].bar(categories, values)
axes[0, 1].set_title('Chart 2')

axes[1, 0].scatter(x2, y2)
axes[1, 0].set_title('Chart 3')

axes[1, 1].hist(data, bins=30)
axes[1, 1].set_title('Chart 4')

plt.tight_layout()
plt.show()
```

### Customization
```python
# Styling your plots
plt.style.use('seaborn-v0_8-darkgrid')

# Custom colors
custom_colors = ['#9B59B6', '#8E44AD', '#7D3C98']

# Annotations
plt.annotate('Peak Sales', xy=(4, 190), xytext=(3, 180),
             arrowprops=dict(facecolor='purple', shrink=0.05))
```

---

## ‚úÖ Key Takeaways
- Visualization is crucial for data communication
- Choose charts based on data type and message
- Matplotlib for control, Seaborn for beauty, Plotly for interactivity
- <span style="background-color: #E6D5FF; padding: 2px 8px; border-radius: 3px;">**Simplicity and clarity**</span> trump complexity

---
"""

# Cell 4 - Markdown
"""
<a id='section3'></a>
# 3. Data Extraction, Loading, Wrangling & Visualization

## Overview
This section covers the <span style="background-color: #E6D5FF; padding: 2px 8px; border-radius: 3px;">**complete data pipeline**</span> - from acquiring raw data to generating actionable insights through systematic transformation and visualization.

---

## üîÑ The Data Pipeline (ETL Process)
```
EXTRACT ‚Üí TRANSFORM ‚Üí LOAD ‚Üí ANALYZE ‚Üí VISUALIZE
```

### Pipeline Stages
1. **Extraction**: Acquire data from various sources
2. **Loading**: Import into Python environment
3. **Wrangling**: Clean, transform, reshape data
4. **Visualization**: Create meaningful representations

---

## üì• Data Extraction

### Understanding Data Sources

| **Source Type** | **Format** | **Extraction Method** |
|----------------|------------|-----------------------|
| Files | CSV, Excel, JSON | Pandas, openpyxl |
| Databases | SQL, NoSQL | SQLAlchemy, pymongo |
| APIs | JSON, XML | requests, urllib |
| Web Pages | HTML | BeautifulSoup, Scrapy |
| Cloud Storage | S3, GCS | boto3, google-cloud |

### **1. File-Based Extraction**

#### CSV Files
```python
import pandas as pd

# Basic CSV reading
df = pd.read_csv('sales_data.csv')

# Advanced CSV reading with parameters
df = pd.read_csv('sales_data.csv',
                 encoding='utf-8',
                 delimiter=',',
                 parse_dates=['Date'],
                 na_values=['NA', 'null', ''],
                 dtype={'ProductID': str},
                 nrows=1000)  # Read first 1000 rows only
```

#### Excel Files
```python
# Single sheet
df = pd.read_excel('financial_report.xlsx', sheet_name='Q1_Sales')

# Multiple sheets
excel_file = pd.ExcelFile('annual_report.xlsx')
sheets_dict = {sheet: excel_file.parse(sheet) for sheet in excel_file.sheet_names}
```

#### JSON Files
```python
# Simple JSON
df = pd.read_json('customers.json')

# Nested JSON
import json
with open('complex_data.json', 'r') as f:
    data = json.load(f)
df = pd.json_normalize(data, record_path='transactions')
```

### **2. API-Based Extraction**
```python
import requests
import pandas as pd

# REST API call
response = requests.get('https://api.example.com/data',
                       headers={'Authorization': 'Bearer TOKEN'},
                       params={'limit': 100, 'offset': 0})

# Check response status
if response.status_code == 200:
    data = response.json()
    df = pd.DataFrame(data['results'])
else:
    print(f"Error: {response.status_code}")

# Pagination handling
def fetch_all_pages(base_url, params):
    all_data = []
    page = 1
    while True:
        params['page'] = page
        response = requests.get(base_url, params=params)
        data = response.json()
        if not data['results']:
            break
        all_data.extend(data['results'])
        page += 1
    return pd.DataFrame(all_data)
```

### **3. Web Scraping**
```python
from bs4 import BeautifulSoup
import requests

# Scrape webpage
url = 'https://example.com/products'
response = requests.get(url)
soup = BeautifulSoup(response.content, 'html.parser')

# Extract data
products = []
for item in soup.find_all('div', class_='product'):
    product = {
        'name': item.find('h2').text,
        'price': item.find('span', class_='price').text,
        'rating': item.find('div', class_='rating')['data-score']
    }
    products.append(product)

df = pd.DataFrame(products)
```

### **4. Database Extraction**
```python
import sqlite3
from sqlalchemy import create_engine

# SQLite connection
conn = sqlite3.connect('business.db')
df = pd.read_sql_query("SELECT * FROM customers WHERE status='active'", conn)
conn.close()

# PostgreSQL/MySQL using SQLAlchemy
engine = create_engine('postgresql://user:password@localhost:5432/dbname')
df = pd.read_sql_table('orders', engine)

# Complex query with joins
query = """
SELECT o.order_id, c.customer_name, p.product_name, o.quantity
FROM orders o
JOIN customers c ON o.customer_id = c.customer_id
JOIN products p ON o.product_id = p.product_id
WHERE o.order_date >= '2024-01-01'
"""
df = pd.read_sql_query(query, engine)
```

---

## üîß Data Wrangling (The Most Time-Consuming Step)

> <span style="background-color: #E6D5FF; padding: 2px 8px; border-radius: 3px;">**80% of data science is data wrangling**</span> - cleaning, transforming, and preparing data for analysis.

### **Phase 1: Initial Data Exploration**
```python
# Load data
df = pd.read_csv('messy_sales.csv')

# Quick overview
print(df.head())           # First 5 rows
print(df.tail())           # Last 5 rows
print(df.info())           # Column types & null counts
print(df.describe())       # Statistical summary
print(df.shape)            # (rows, columns)

# Check for issues
print(df.isnull().sum())   # Count missing values per column
print(df.duplicated().sum())  # Count duplicate rows
print(df.dtypes)           # Data types
```

### **Phase 2: Handling Missing Data**
```python
# Identify missing data patterns
import missingno as msno
msno.matrix(df)  # Visualize missing data

# Strategy 1: Remove missing data
df_clean = df.dropna()  # Drop rows with ANY missing values
df_clean = df.dropna(subset=['Revenue', 'Date'])  # Drop only if specific columns missing
df_clean = df.dropna(thresh=5)  # Keep rows with at least 5 non-null values

# Strategy 2: Fill missing data
df['Age'].fillna(df['Age'].mean(), inplace=True)  # Mean imputation
df['Category'].fillna(df['Category'].mode()[0], inplace=True)  # Mode for categorical
df['Price'].fillna(method='ffill', inplace=True)  # Forward fill
df['Price'].fillna(method='bfill', inplace=True)  # Backward fill

# Strategy 3: Advanced imputation
from sklearn.impute import SimpleImputer, KNNImputer

# Using KNN imputation
imputer = KNNImputer(n_neighbors=5)
df[['Age', 'Income']] = imputer.fit_transform(df[['Age', 'Income']])
```

### **Phase 3: Data Type Conversion**
```python
# Convert data types
df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')
df['Revenue'] = pd.to_numeric(df['Revenue'], errors='coerce')  # Invalid ‚Üí NaN
df['CustomerID'] = df['CustomerID'].astype(str)
df['Category'] = df['Category'].astype('category')  # Memory efficient

# Handle currency/percentage strings
df['Price'] = df['Price'].str.replace('$', '').str.replace(',', '').astype(float)
df['Growth'] = df['Growth'].str.rstrip('%').astype(float) / 100
```

### **Phase 4: Handling Duplicates**
```python
# Identify duplicates
print(df.duplicated().sum())
print(df[df.duplicated(keep=False)])  # Show all duplicates

# Remove duplicates
df_unique = df.drop_duplicates()
df_unique = df.drop_duplicates(subset=['CustomerID'], keep='last')  # Keep most recent
df_unique = df.drop_duplicates(subset=['Email', 'Phone'], keep='first')
```

### **Phase 5: Outlier Detection & Treatment**
```python
import numpy as np

# Statistical method: IQR (Interquartile Range)
Q1 = df['Revenue'].quantile(0.25)
Q3 = df['Revenue'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Identify outliers
outliers = df[(df['Revenue'] < lower_bound) | (df['Revenue'] > upper_bound)]
print(f"Found {len(outliers)} outliers")

# Treatment options
df_no_outliers = df[(df['Revenue'] >= lower_bound) & (df['Revenue'] <= upper_bound)]  # Remove
df['Revenue_capped'] = np.clip(df['Revenue'], lower_bound, upper_bound)  # Cap values

# Z-score method
from scipy import stats
z_scores = np.abs(stats.zscore(df[['Revenue', 'Quantity']]))
df_no_outliers = df[(z_scores < 3).all(axis=1)]  # Keep values within 3 std devs
```

### **Phase 6: Feature Engineering**
```python
# Extract date components
df['Year'] = df['Date'].dt.year
df['Month'] = df['Date'].dt.month
df['Quarter'] = df['Date'].dt.quarter
df['DayOfWeek'] = df['Date'].dt.day_name()
df['IsWeekend'] = df['Date'].dt.dayofweek.isin([5, 6]).astype(int)

# Create calculated fields
df['TotalRevenue'] = df['Quantity'] * df['Price']
df['ProfitMargin'] = (df['Revenue'] - df['Cost']) / df['Revenue'] * 100
df['RevenuePerCustomer'] = df.groupby('CustomerID')['Revenue'].transform('mean')

# Binning continuous variables
df['AgeGroup'] = pd.cut(df['Age'], bins=[0, 18, 35, 50, 65, 100],
                         labels=['<18', '18-35', '36-50', '51-65', '65+'])
df['RevenueCategory'] = pd.qcut(df['Revenue'], q=4, 
                                 labels=['Low', 'Medium', 'High', 'VeryHigh'])

# Encoding categorical variables
df_encoded = pd.get_dummies(df, columns=['Category', 'Region'], prefix=['Cat', 'Reg'])

# Label encoding for ordinal data
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['Priority_Encoded'] = le.fit_transform(df['Priority'])  # Low, Medium, High ‚Üí 0, 1, 2
```

### **Phase 7: Data Transformation & Reshaping**
```python
# Pivoting: Wide to long format
df_long = pd.melt(df, id_vars=['Date', 'Region'], 
                  value_vars=['Product_A', 'Product_B', 'Product_C'],
                  var_name='Product', value_name='Sales')

# Pivot table: Long to wide format
df_wide = df_long.pivot_table(index='Date', columns='Product', 
                               values='Sales', aggfunc='sum')

# Grouping and aggregation
summary = df.groupby('Category').agg({
    'Revenue': ['sum', 'mean', 'count'],
    'Quantity': 'sum',
    'CustomerID': 'nunique'
}).round(2)

# Multi-level grouping
regional_summary = df.groupby(['Region', 'Category']).agg({
    'Revenue': 'sum',
    'Quantity': 'mean'
}).reset_index()

# Rolling calculations (moving averages)
df['Revenue_MA_7'] = df['Revenue'].rolling(window=7).mean()
df['Revenue_MA_30'] = df['Revenue'].rolling(window=30).mean()

# Sorting
df_sorted = df.sort_values(by=['Region', 'Revenue'], ascending=[True, False])
```

### **Phase 8: Merging & Joining Datasets**
```python
# Inner join (intersection)
merged = pd.merge(customers, orders, on='CustomerID', how='inner')

# Left join (all from left, matching from right)
merged = pd.merge(customers, orders, on='CustomerID', how='left')

# Multiple key join
merged = pd.merge(df1, df2, left_on=['ID', 'Date'], right_on=['CustomerID', 'OrderDate'])

# Concatenating dataframes
df_combined = pd.concat([df_2022, df_2023, df_2024], axis=0, ignore_index=True)
```

---

## üìä Advanced Visualization Integration

### **Complete Analysis Pipeline Example**
```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 1. EXTRACT
df = pd.read_csv('sales_data.csv')

# 2. WRANGLE
# Clean data
df['Date'] = pd.to_datetime(df['Date'])
df = df.dropna(subset=['Revenue', 'Region'])
df = df[df['Revenue'] > 0]  # Remove invalid entries

# Feature engineering
df['Month'] = df['Date'].dt.to_period('M')
df['Revenue_K'] = df['Revenue'] / 1000

# 3. AGGREGATE
monthly_sales = df.groupby(['Month', 'Region'])['Revenue_K'].sum().reset_index()
monthly_sales['Month'] = monthly_sales['Month'].astype(str)

# 4. VISUALIZE
fig, axes = plt.subplots(2, 2, figsize=(16, 12))
fig.suptitle('Comprehensive Sales Analysis Dashboard', fontsize=18, fontweight='bold')

# Plot 1: Trend over time
for region in monthly_sales['Region'].unique():
    region_data = monthly_sales[monthly_sales['Region'] == region]
    axes[0, 0].plot(region_data['Month'], region_data['Revenue_K'], 
                    marker='o', label=region, linewidth=2)
axes[0, 0].set_title('Monthly Revenue Trends by Region', fontweight='bold')
axes[0, 0].set_xlabel('Month')
axes[0, 0].set_ylabel('Revenue ($1000s)')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)
axes[0, 0].tick_params(axis='x', rotation=45)

# Plot 2: Distribution
axes[0, 1].hist(df['Revenue_K'], bins=50, color='purple', alpha=0.7, edgecolor='black')
axes[0, 1].set_title('Revenue Distribution', fontweight='bold')
axes[0, 1].set_xlabel('Revenue ($1000s)')
axes[0, 1].set_ylabel('Frequency')
axes[0, 1].axvline(df['Revenue_K'].mean(), color='red', linestyle='--', 
                   label=f"Mean: ${df['Revenue_K'].mean():.1f}K")
axes[0, 1].legend()

# Plot 3: Regional comparison
regional_total = df.groupby('Region')['Revenue_K'].sum().sort_values(ascending=False)
axes[1, 0].barh(regional_total.index, regional_total.values, 
                color=['#9B59B6', '#8E44AD', '#7D3C98', '#6C3483'])
axes[1, 0].set_title('Total Revenue by Region', fontweight='bold')
axes[1, 0].set_xlabel('Revenue ($1000s)')

# Plot 4: Correlation heatmap
numeric_cols = df.select_dtypes(include=[np.number]).columns
correlation = df[numeric_cols].corr()
sns.heatmap(correlation, annot=True, fmt='.2f', cmap='Purples', 
            ax=axes[1, 1], cbar_kws={'label': 'Correlation'})
axes[1, 1].set_title('Feature Correlation Matrix', fontweight='bold')

plt.tight_layout()
plt.show()
```

### **Interactive Dashboard with Plotly**
```python
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Create subplots
fig = make_subplots(
    rows=2, cols=2,
    subplot_titles=('Revenue Trend', 'Category Distribution', 
                    'Regional Performance', 'Growth Rate'),
    specs=[[{'type': 'scatter'}, {'type': 'bar'}],
           [{'type': 'bar'}, {'type': 'scatter'}]]
)

# Add traces
fig.add_trace(go.Scatter(x=df['Date'], y=df['Revenue'], 
                         mode='lines+markers', name='Revenue',
                         line=dict(color='purple', width=3)),
              row=1, col=1)

fig.add_trace(go.Bar(x=category_data['Category'], y=category_data['Count'],
                     marker_color='mediumpurple', name='Categories'),
              row=1, col=2)

# Update layout
fig.update_layout(height=800, showlegend=True, 
                  title_text="<b>Interactive Sales Dashboard</b>",
                  title_font_size=20)

fig.show()
```

---

## üéØ Real-World Case Study

### Problem Statement
Analyze e-commerce sales data to identify growth opportunities and optimize inventory.
```python
# STEP 1: Extract from multiple sources
orders = pd.read_csv('orders.csv')
products = pd.read_excel('products.xlsx')
customers_api = requests.get('https://api.company.com/customers').json()
customers = pd.DataFrame(customers_api)

# STEP 2: Data wrangling
# Merge datasets
df = orders.merge(products, on='ProductID').merge(customers, on='CustomerID')

# Clean and transform
df['OrderDate'] = pd.to_datetime(df['OrderDate'])
df['Revenue'] = df['Quantity'] * df['UnitPrice']
df = df[df['Revenue'] > 0]  # Remove invalid orders

# Feature engineering
df['Month'] = df['OrderDate'].dt.to_period('M')
df['CustomerLifetimeValue'] = df.groupby('CustomerID')['Revenue'].transform('sum')
df['IsHighValue'] = (df['CustomerLifetimeValue'] > df['CustomerLifetimeValue'].quantile(0.75)).astype(int)

# STEP 3: Analysis
# Top products
top_products = df.groupby('ProductName').agg({
    'Revenue': 'sum',
    'Quantity': 'sum',
    'OrderID': 'count'
}).sort_values('Revenue', ascending=False).head(10)

# Customer segmentation
segments = df.groupby('IsHighValue').agg({
    'CustomerID': 'nunique',
    'Revenue': 'mean',
    'OrderID': 'count'
})

# Growth analysis
monthly_growth = df.groupby('Month')['Revenue'].sum().pct_change() * 100

# STEP 4: Visualization
fig, axes = plt.subplots(2, 2, figsize=(16, 10))

# Top products
axes[0, 0].barh(top_products.index, top_products['Revenue'], color='purple')
axes[0, 0].set_title('Top 10 Products by Revenue', fontweight='bold')

# Monthly trend
axes[0, 1].plot(monthly_growth.index.astype(str), monthly_growth.values, 
                marker='o', color='purple', linewidth=2)
axes[0, 1].set_title('Month-over-Month Growth Rate', fontweight='bold')
axes[0, 1].axhline(y=0, color='red', linestyle='--')

# Customer segmentation
segments['Revenue'].plot(kind='bar', ax=axes[1, 0], color=['lightblue', 'purple'])
axes[1, 0].set_title('Average Revenue by Customer Segment', fontweight='bold')

# Revenue distribution
axes[1, 1].scatter(df['Quantity'], df['Revenue'], alpha=0.5, color='purple')
axes[1, 1].set_title('Quantity vs Revenue Relationship', fontweight='bold')

plt.tight_layout()
plt.show()

# STEP 5: Export insights
top_products.to_excel('top_products_report.xlsx')
print("\n‚úÖ Analysis complete! Key findings:")
print(f"- Total revenue: ${df['Revenue'].sum():,.2f}")
print(f"- High-value customers: {segments.loc[1, 'CustomerID']} ({segments.loc[1, 'CustomerID']/df['CustomerID'].nunique()*100:.1f}%)")
print(f"- Average monthly growth: {monthly_growth.mean():.2f}%")
```

---

## üî• Advanced Techniques

### **1. Working with Large Datasets**
```python
# Chunk processing for memory efficiency
chunk_size = 10000
chunks = pd.read_csv('huge_file.csv', chunksize=chunk_size)

results = []
for chunk in chunks:
    # Process each chunk
    processed = chunk[chunk['Value'] > 0].groupby('Category')['Value'].sum()
    results.append(processed)

final_result = pd.concat(results).groupby(level=0).sum()

# Using Dask for parallel processing
import dask.dataframe as dd
ddf = dd.read_csv('huge_file.csv')
result = ddf.groupby('Category')['Value'].mean().compute()
```

### **2. Time Series Analysis**
```python
# Resampling
df.set_index('Date', inplace=True)
weekly = df['Revenue'].resample('W').sum()
monthly = df['Revenue'].resample('M').mean()

# Seasonality decomposition
from statsmodels.tsa.seasonal import seasonal_decompose
decomposition = seasonal_decompose(df['Revenue'], model='multiplicative', period=12)

fig, axes = plt.subplots(4, 1, figsize=(12, 10))
decomposition.observed.plot(ax=axes[0], title='Observed')
decomposition.trend.plot(ax=axes[1], title='Trend')
decomposition.seasonal.plot(ax=axes[2], title='Seasonality')
decomposition.resid.plot(ax=axes[3], title='Residuals')
plt.tight_layout()
```

### **3. Statistical Analysis Integration**
```python
from scipy import stats

# Hypothesis testing
group_a = df[df['Group'] == 'A']['Revenue']
group_b = df[df['Group'] == 'B']['Revenue']
t_stat, p_value = stats.ttest_ind(group_a, group_b)

if p_value < 0.05:
    print(f"<span style='background-color: #E6D5FF'>Significant difference found (p={p_value:.4f})</span>")

# Correlation analysis
correlation, p_val = stats.pearsonr(df['Marketing_Spend'], df['Revenue'])
print(f"Correlation: {correlation:.3f} (p-value: {p_val:.4f})")
```

---

## ‚úÖ Complete Workflow Checklist

### <span style="background-color: #E6D5FF; padding: 2px 8px; border-radius: 3px;">**Data Extraction**</span>
- [ ] Identify data sources (files, APIs, databases, web)
- [ ] Choose appropriate extraction method
- [ ] Handle authentication and permissions
- [ ] Implement error handling and logging

### <span style="background-color: #E6D5FF; padding: 2px 8px; border-radius: 3px;">**Data Loading**</span>
- [ ] Load data into pandas DataFrame
- [ ] Verify data structure and shape
- [ ] Check initial data quality

### <span style="background-color: #E6D5FF; padding: 2px 8px; border-radius: 3px;">**Data Wrangling**</span>
- [ ] Explore data (head, info, describe)
- [ ] Handle missing values
- [ ] Remove/treat duplicates
- [ ] Convert data types
- [ ] Detect and handle outliers
- [ ] Engineer relevant features
- [ ] Merge/join datasets if needed
- [ ] Create aggregations

### <span style="background-color: #E6D5FF; padding: 2px 8px; border-radius: 3px;">**Data Visualization**</span>
- [ ] Choose appropriate chart types
- [ ] Create exploratory visualizations
- [ ] Design dashboard layout
- [ ] Add titles, labels, legends
- [ ] Apply consistent styling
- [ ] Export/save visualizations

---

## üéì Key Takeaways

1. **Data quality determines analysis quality** - invest time in thorough wrangling
2. **Automate repetitive tasks** - build reusable functions and pipelines
3. **Document your process** - code comments and markdown cells
4. **Validate at each step** - check data after transformations
5. **<span style="background-color: #E6D5FF; padding: 2px 8px; border-radius: 3px;">Iterate and refine</span>** - data work is rarely perfect on first attempt

---

## üìö Additional Resources

- **Pandas Documentation**: https://pandas.pydata.org/docs/
- **Matplotlib Gallery**: https://matplotlib.org/stable/gallery/
- **Seaborn Tutorial**: https://seaborn.pydata.org/tutorial.html
- **Real Python Tutorials**: https://realpython.com
- **Kaggle Datasets**: https://www.kaggle.com/datasets

---
"""

# Cell 5 - Markdown
"""
# üéâ Congratulations!

You've completed the comprehensive guide covering:
1. ‚úÖ Why Python is essential for business and ML
2. ‚úÖ Data visualization fundamentals and libraries
3. ‚úÖ Complete ETL pipeline with advanced wrangling techniques

## Next Steps
- Practice with real datasets from Kaggle or UCI Repository
- Build your own analysis projects
- Explore machine learning with scikit-learn
- Create interactive dashboards with Streamlit or Dash

<div style="background-color: #E6D5FF; padding: 20px; border-radius: 10px; margin-top: 20px;">
<h3 style="margin-top: 0;">üí° Remember</h3>
<p><b>The best way to learn is by doing!</b> Take these concepts and apply them to your own data problems.</p>
</div>

---

*Happy Coding! üêçüìä*
"""
